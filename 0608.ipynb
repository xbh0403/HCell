{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import scanpy as sc\n",
    "from anndata.experimental.pytorch import AnnLoader\n",
    "\n",
    "import pretty_confusion_matrix as pcm\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchnet.meter import ClassErrorMeter, AverageValueMeter\n",
    "# from torch_prototypes.modules import prototypical_network\n",
    "import prototypical_network\n",
    "from torch_prototypes.metrics import distortion, cost\n",
    "from torch_prototypes.metrics.distortion import DistortionLoss\n",
    "from  torch.distributions import multivariate_normal\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------Parameters-------------------\n",
    "embedding_dim = 3\n",
    "k_fold = 5\n",
    "cross_validation = False\n",
    "num_epoch=10\n",
    "batch_size=512\n",
    "feature_selection = True\n",
    "num_genes = 36601\n",
    "# --------------Plotting---------------------\n",
    "plot_loss = True\n",
    "plot_embedding_space = True\n",
    "plot_confusion_matrix = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        # First fully connected layer\n",
    "        self.fc1 = nn.Linear(num_genes, 512)\n",
    "        # Second fully connected layer that outputs our 10 labels\n",
    "        self.drop1 = nn.Dropout(p=0.3)\n",
    "        self.fc2 = nn.Linear(512, embedding_dim)\n",
    "        self.cuda()\n",
    "\n",
    "    def forward(self, x):\n",
    "      x = self.fc1(x)\n",
    "      x = F.relu(x)\n",
    "      x = self.drop1(x)\n",
    "      x = self.fc2(x)\n",
    "\n",
    "      return x\n",
    "\n",
    "\n",
    "class PL(nn.Module):\n",
    "    def __init__(self, centers):\n",
    "        super(PL, self).__init__()\n",
    "        self.centers = centers\n",
    "\n",
    "    def forward(self, mapping, labels):\n",
    "        targets = torch.index_select(self.centers, 0, labels)\n",
    "        dist = torch.norm(mapping - targets, dim=1)\n",
    "        # print(dist[0])\n",
    "        dist = torch.sum(dist)\n",
    "        return dist/mapping.shape[0]\n",
    "\n",
    "class I2CS(nn.Module):\n",
    "    def __init__(self, centers):\n",
    "        super(I2CS, self).__init__()\n",
    "        self.centers = centers\n",
    "\n",
    "    def forward(self, mapping, labels):\n",
    "        sum_intra = torch.Tensor(0)\n",
    "        sum_inter = torch.Tensor(0)\n",
    "        for i in range(len(self.centers)):\n",
    "            mask = labels == i\n",
    "            ind = torch.nonzero(mask)\n",
    "            list_one_type = mapping[ind]\n",
    "            dist = torch.dist(list_one_type - self.centers[i])\n",
    "            avg = torch.mean(torch.sum(dist))\n",
    "            sum_intra += avg\n",
    "        for i in range(len(self.centers)):\n",
    "            dists = torch.dist(self.centers[i] - self.centers)\n",
    "            min_dist = torch.min(dists)\n",
    "            sum_inter+= min_dist\n",
    "        return sum_intra/sum_inter\n",
    "\n",
    "# class PL_Inter(nn.Module):\n",
    "#     def __init__(self, centers):\n",
    "#         super(PL_Inter, self).__init__()\n",
    "#         self.centers = centers\n",
    "\n",
    "#     def forward(self):\n",
    "#         targets = torch.index_select(self.centers, 0, labels)\n",
    "#         dist = torch.norm(mapping - targets, dim=1)\n",
    "#         # print(dist[0])\n",
    "#         dist = torch.sum(dist)\n",
    "#         return dist/mapping.shape[0]\n",
    "\n",
    "# class PL_Norm(nn.Module):\n",
    "#     def __init__(self, centers):\n",
    "#         super(PL_Norm, self).__init__()\n",
    "#         self.centers = centers\n",
    "#         self.dists = []\n",
    "#         for center in centers:\n",
    "#             self.dists.append(multivariate_normal.MultivariateNormal(loc=center.detach().cpu(), covariance_matrix=torch.eye(embedding_dim).detach().cpu()))\n",
    "\n",
    "#     def forward(self, mapping, label):\n",
    "#         sum = 0\n",
    "#         # max = 0\n",
    "#         for i, x in enumerate(mapping):\n",
    "#             likelihood = torch.exp(self.dists[label[i]].log_prob(x.detach().cpu()))\n",
    "#             # max_likelihood = torch.exp(self.dists[label[i]].log_prob(self.centers[label[i]].detach().cpu()))\n",
    "#             # max+=max_likelihood\n",
    "#             sum+=likelihood\n",
    "#         return -sum/mapping.shape[0]\n",
    "\n",
    "def create_model(D_metric=None, cuda=1):\n",
    "  device = 'cuda' if cuda else 'cpu'\n",
    "  D_metric = D_metric.cuda() if cuda and D_metric is not None else D_metric\n",
    "  model_embedding = SimpleNN() \n",
    "  model = prototypical_network.LearntPrototypes(model_embedding, n_prototypes= D_metric.shape[0],\n",
    "                                prototypes=None, embedding_dim=embedding_dim, device=device)\n",
    "  return model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = pd.read_csv('C:/Users/xbh04/Desktop/distance_matrix_bcell_ABCs.csv').iloc[:, 1:]\n",
    "D = torch.tensor(D.values, dtype=float)\n",
    "dataset = sc.read_h5ad(\"C:/Users/xbh04/Desktop/b-cells.h5ad\")\n",
    "dataset = dataset[dataset.obs['Manually_curated_celltype'] != 'MNP/B doublets']\n",
    "dataset = dataset[dataset.obs['Manually_curated_celltype'] != 'T/B doublets']\n",
    "dataset = dataset[dataset.obs['Manually_curated_celltype'] != 'ABCs']\n",
    "dataset_Pro_B = dataset[dataset.obs['Manually_curated_celltype'] == 'Pro-B']\n",
    "dataset = dataset[dataset.obs['Manually_curated_celltype'] != 'Pro-B']\n",
    "encoder_celltype = LabelEncoder()\n",
    "encoder_celltype.fit(dataset.obs['Manually_curated_celltype'])\n",
    "encoders = {\n",
    "    'obs': {\n",
    "        'Manually_curated_celltype': encoder_celltype.transform\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split & Cross-validation\n",
    "def costumized_train_test_split(dataset, encoders, test_size=0.2):\n",
    "    indices_by_celltypes = {}\n",
    "    train_indices, test_indices, cv = [], [], []\n",
    "    for cell_type in dataset.obs['Manually_curated_celltype'].unique():\n",
    "        indices = np.where(dataset.obs['Manually_curated_celltype'] == cell_type)[0]\n",
    "        np.random.shuffle(indices)\n",
    "        indices_by_celltypes.update({cell_type: indices})\n",
    "        split = int(len(indices)/k_fold)\n",
    "        if cross_validation:\n",
    "            for i in range(k_fold):\n",
    "                temp = i*split\n",
    "                temp_test = list(indices[temp:temp+split])\n",
    "                temp_train = list(set(indices) - set(temp_test))\n",
    "                if cell_type != dataset.obs['Manually_curated_celltype'].unique()[0]:\n",
    "                    cv[i].get(\"train\").extend(temp_train)\n",
    "                    cv[i].get(\"test\").extend(temp_test)\n",
    "                else:\n",
    "                    cv.append({\"train\":temp_train, \"test\": temp_test})\n",
    "        else:\n",
    "            test_indices.extend(indices[:split])\n",
    "            train_indices.extend(indices[split:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection by Scanpy\n",
    "def select_features(dataset_training):\n",
    "    print(\"feature_selection\")\n",
    "    dataset_training.var['mt'] = dataset_training.var_names.str.startswith('MT-')  # annotate the group of mitochondrial genes as 'mt'\n",
    "    sc.pp.calculate_qc_metrics(dataset_training, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True)\n",
    "    sc_pp_train = sc.pp.filter_cells(dataset_training, min_genes=200, copy=True)\n",
    "    sc.pp.filter_genes(sc_pp_train, min_cells=3)\n",
    "    sc_pp_train = sc_pp_train[sc_pp_train.obs.n_genes_by_counts < 2500, :]\n",
    "    sc_pp_train = sc_pp_train[sc_pp_train.obs.pct_counts_mt < 5, :]\n",
    "    sc.pp.highly_variable_genes(sc_pp_train, n_top_genes=int(num_genes/4))\n",
    "    sc_pp_train = sc_pp_train[:, sc_pp_train.var.highly_variable]\n",
    "    return sc_pp_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(l_metric=2, l_pl = 2, epochs = num_epoch, D_metric=None, D_cost=D, cuda=1):\n",
    "    D_metric = D_metric.cuda()\n",
    "    model = create_model(D_metric, cuda)      \n",
    "    dl_train = dataloader_training\n",
    "    dl_test = dataloader_testing\n",
    "    \n",
    "    delta = DistortionLoss(D_metric)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    ac = cost.AverageCost(D_cost)\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters())\n",
    "    # log = {}\n",
    "    if plot_loss:\n",
    "        loss_xe = []\n",
    "        loss_disto = []\n",
    "        loss_pl = []\n",
    "    if plot_embedding_space:\n",
    "        training_embeddings = []\n",
    "        training_labels = []\n",
    "        testing_embeddings = []\n",
    "        testing_pred_labels = []\n",
    "        testing_true_labels = []\n",
    "    for epoch in range(1, epochs+1):\n",
    "        print('Epoch {}'.format(epoch))\n",
    "        ER_meter = ClassErrorMeter(accuracy=False)\n",
    "        AC_meter = AverageValueMeter()\n",
    "\n",
    "        model.train()\n",
    "        t0 = time.time()\n",
    "        if plot_loss:\n",
    "            batch_xe = []\n",
    "            batch_disto = []\n",
    "            batch_pl = []\n",
    "        for batch in dl_train:\n",
    "            x = batch.X.cuda()\n",
    "            y = batch.obs['Manually_curated_celltype'].type(torch.LongTensor).cuda()\n",
    "            y = y.squeeze()\n",
    "            y.long()\n",
    "            out, embeddings = model(x)\n",
    "            opt.zero_grad()\n",
    "            pl_loss = PL(centers = model.prototypes.data)\n",
    "            i2cs = I2CS(centers = model.prototypes.data)\n",
    "            pl_loss_ = pl_loss(embeddings, y)\n",
    "            i2cs_loss_ = i2cs(embeddings, y)\n",
    "            print(i2cs_loss_)\n",
    "            if epoch == epochs and plot_embedding_space:\n",
    "                training_embeddings.extend(embeddings)\n",
    "                training_labels.extend(y)\n",
    "            # loss = criterion(out, y)  + l_pl*pl_loss_\n",
    "            loss = criterion(out, y) +  l_metric * delta(model.prototypes) + l_pl*pl_loss_\n",
    "            \n",
    "            if plot_loss:\n",
    "                batch_xe.append(criterion(out, y).detach().cpu())\n",
    "                batch_disto.append((l_metric * delta(model.prototypes)).detach().cpu())\n",
    "                batch_pl.append(l_pl*pl_loss_.detach().cpu())\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            pred = out.detach()\n",
    "            ER_meter.add(pred.cpu(),y.cpu())\n",
    "            AC_meter.add(ac(pred.cpu(),y.cpu()))\n",
    "        \n",
    "        if plot_loss:\n",
    "            loss_xe.append(np.array(batch_xe).mean())\n",
    "            loss_disto.append(np.array(batch_disto).mean())\n",
    "            loss_pl.append(np.array(batch_pl).mean())\n",
    "            \n",
    "        t1 = time.time()\n",
    "        # log[epoch] = {'train_ER':ER_meter.value()[0], 'train_AC':AC_meter.value()[0], 'train_time':t1-t0}\n",
    "        \n",
    "        print('Train ER {:.2f}, AC {:.3f}, time {:.1f}s'.format(ER_meter.value()[0], AC_meter.value()[0], t1-t0))\n",
    "\n",
    "        model.eval()\n",
    "        ER_meter = ClassErrorMeter(accuracy=False)\n",
    "        AC_meter = AverageValueMeter()\n",
    "        t0 = time.time()\n",
    "        for batch in dl_test:\n",
    "            x = batch.X.cuda()\n",
    "            y = batch.obs['Manually_curated_celltype'].type(torch.LongTensor).cuda()\n",
    "            y = y.squeeze()\n",
    "            y.long()\n",
    "            with torch.no_grad():\n",
    "                out, embedding_y = model(x)\n",
    "            pred = out.detach()\n",
    "            # pred = out\n",
    "            if epoch == epochs and plot_embedding_space:\n",
    "                testing_embeddings.extend(embedding_y)\n",
    "                testing_pred_labels.extend(pred.cpu().numpy())\n",
    "                testing_true_labels.extend(y)\n",
    "            ER_meter.add(pred.cpu(),y)\n",
    "            AC_meter.add(ac(pred.cpu(),y))\n",
    "        t1 = time.time()\n",
    "        print('Test ER {:.2f}, AC {:.3f}, time {:.1f}s'.format(ER_meter.value()[0], AC_meter.value()[0], t1-t0))\n",
    "        # log[epoch].update({'test_ER':ER_meter.value()[0], 'test_AC':AC_meter.value()[0], 'test_time':t1-t0})\n",
    "        results = {}\n",
    "        results['model'] = model\n",
    "        if plot_loss:\n",
    "            results['loss_xe'] = loss_xe\n",
    "            results['loss_disto'] = loss_disto\n",
    "            results['loss_pl'] = loss_pl\n",
    "        if plot_embedding_space:\n",
    "            results['training_embeddings'] = training_embeddings\n",
    "            results['training_labels'] = training_labels\n",
    "            results['test_embeddings'] = testing_embeddings\n",
    "            results['test_true_labels'] = testing_true_labels\n",
    "            results['test_pred_labels'] = testing_pred_labels\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if cross_validation:\n",
    "#     results_list = []\n",
    "#     for fold in range(k_fold):\n",
    "#         print(f'FOLD {fold}')\n",
    "#         print('--------------------------------')\n",
    "#         if feature_selection:\n",
    "#             train_dataset = select_features(train_dataset)\n",
    "#             test_dataset = test_dataset[:,train_dataset.var_names]\n",
    "#             num_genes = len(train_dataset.var_names)\n",
    "#         # Define data loaders for training and testing data in this fold\n",
    "\n",
    "#         train_subsampler = torch.utils.data.SubsetRandomSampler(cv[fold]['train'])\n",
    "#         test_subsampler = torch.utils.data.SubsetRandomSampler(cv[fold]['test'])\n",
    "#         dataloader_training = AnnLoader(train_dataset, batch_size=batch_size, convert=encoders, sampler=train_subsampler)\n",
    "#         dataloader_testing = AnnLoader(test_dataset, batch_size=batch_size, convert=encoders, sampler=test_subsampler)\n",
    "\n",
    "#         results = train_nn(D_metric=D, l_metric=1)\n",
    "#         results_list.append(results)\n",
    "# else:\n",
    "    \n",
    "#     if feature_selection:\n",
    "#         train_dataset = dataset[train_indices]\n",
    "#         train_dataset = select_features(train_dataset)\n",
    "#         dataset = dataset[:,train_dataset.var_names]\n",
    "#         num_genes = len(train_dataset.var_names)\n",
    "        \n",
    "#     # Define data loaders for training and testing data in this fold\n",
    "#     dataloader_training = AnnLoader(dataset, batch_size=batch_size, convert=encoders, sampler=train_indices)\n",
    "#     dataloader_testing = AnnLoader(dataset, batch_size=batch_size, convert=encoders, sampler=test_indices)\n",
    "\n",
    "#     results = train_nn(D_metric=D, l_metric=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=k_fold, shuffle=True)\n",
    "# Start print\n",
    "print('--------------------------------')\n",
    "model_list = []\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
    "      # Print\n",
    "      print(f'FOLD {fold}')\n",
    "      print('--------------------------------')\n",
    "\n",
    "      # Sample elements randomly from a given list of ids, no replacement.\n",
    "      train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "      test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "      if feature_selection:\n",
    "            train_dataset = dataset[train_indices]\n",
    "            train_dataset = select_features(train_dataset)\n",
    "            dataset = dataset[:,train_dataset.var_names]\n",
    "            num_genes = len(train_dataset.var_names)\n",
    "      # Define data loaders for training and testing data in this fold\n",
    "            \n",
    "      sc.pp.normalize_total(dataset, target_sum = 1e4)\n",
    "      sc.pp.log1p(dataset)\n",
    "      dataloader_training = AnnLoader(dataset, batch_size=batch_size, convert=encoders, sampler=train_subsampler)\n",
    "      dataloader_testing = AnnLoader(dataset, batch_size=batch_size, convert=encoders, sampler=test_subsampler)\n",
    "\n",
    "      results = train_nn(D_metric=D, l_metric=1)\n",
    "      model_list.append(results)\n",
    "      \n",
    "      if not cross_validation:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_loss:\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    ax = plt.axes()\n",
    "\n",
    "    x = np.linspace(0, len(results.get('loss_pl')), len(results.get('loss_pl')))\n",
    "    plt.plot(x, np.array(results.get('loss_xe')), label='xe')\n",
    "    plt.plot(x, np.array(results.get('loss_disto')), label='disto')\n",
    "    plt.plot(x, np.array(results.get('loss_pl')), label='pl')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "training_embeddings = results.get('training_embeddings')\n",
    "training_embeddings_labels = results.get('training_labels')\n",
    "if type(training_embeddings[0]) != np.ndarray:\n",
    "    for i in range(len(training_embeddings_labels)):\n",
    "            training_embeddings[i] = training_embeddings[i].detach().cpu().numpy()\n",
    "            training_embeddings_labels[i] = training_embeddings_labels[i].cpu()\n",
    "training_embeddings_labels=encoder_celltype.inverse_transform(training_embeddings_labels)\n",
    "if plot_embedding_space:\n",
    "    fig, ax = plt.subplots(figsize=(15, 10))\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    for color in np.unique(np.array(training_embeddings_labels)):\n",
    "    # for color in [\"Memory B cells\", \"Naive B cells\"]:\n",
    "        i = np.where(training_embeddings_labels == color)\n",
    "        ax.scatter(np.array(training_embeddings)[i,0], np.array(training_embeddings)[i,1],np.array(training_embeddings)[i,2], label=color)\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "test_embeddings = results.get('test_embeddings')\n",
    "test_true_labels = results.get('test_true_labels')\n",
    "if type(test_embeddings[0]) != np.ndarray:\n",
    "    for i in range(len(test_embeddings)):\n",
    "        test_embeddings[i] = test_embeddings[i].cpu().numpy()\n",
    "        test_true_labels[i] = test_true_labels[i].cpu()\n",
    "test_true_labels=encoder_celltype.inverse_transform(test_true_labels)\n",
    "if plot_embedding_space:\n",
    "    fig, ax = plt.subplots(figsize=(15, 10))\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    for color in np.unique(np.array(test_true_labels)):\n",
    "        i = np.where(test_true_labels == color)\n",
    "        ax.scatter(np.array(test_embeddings)[i,0], np.array(test_embeddings)[i,1],np.array(test_embeddings)[i,2], label=color)\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing pred labels\n",
    "test_embeddings = results.get('test_embeddings')\n",
    "test_pred_labels = results.get('test_pred_labels').copy()\n",
    "for i in range(len(test_pred_labels)):\n",
    "    test_pred_labels[i] = test_pred_labels[i].argmax()\n",
    "\n",
    "test_pred_labels=encoder_celltype.inverse_transform(test_pred_labels)\n",
    "if plot_embedding_space:\n",
    "    fig, ax = plt.subplots(figsize=(15, 10))\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    for color in np.unique(np.array(test_pred_labels)):\n",
    "        i = np.where(test_pred_labels == color)\n",
    "        ax.scatter(np.array(test_embeddings)[i,0], np.array(test_embeddings)[i,1], np.array(test_embeddings)[i,2], label=color)\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_dists = results.get('test_pred_labels').copy()\n",
    "test_pred_labels = encoder_celltype.fit_transform(test_pred_labels)\n",
    "test_true_labels = encoder_celltype.fit_transform(test_true_labels)\n",
    "true_pos = []\n",
    "for i in range(len(test_pred_dists)):\n",
    "    test_pred_dists[i] = -test_pred_dists[i][test_pred_labels[i]]\n",
    "    if test_pred_labels[i] == test_true_labels[i]:\n",
    "        true_pos.append(test_pred_dists[i])\n",
    "\n",
    "g = sns.displot(test_pred_dists)\n",
    "g.fig.set_size_inches(15,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_Pro_B=dataset_Pro_B[:, dataset.var_names]\n",
    "sc.pp.normalize_total(dataset_Pro_B, target_sum = 1e4)\n",
    "sc.pp.log1p(dataset_Pro_B)\n",
    "dataset_Pro_B = AnnLoader(dataset_Pro_B, batch_size=512)\n",
    "for batch in dataset_Pro_B:\n",
    "    x = batch.X.cuda()\n",
    "    model = results.get('model')\n",
    "    with torch.no_grad():\n",
    "        out_Pro_B, embedding_Pro_B = model(x)\n",
    "    pred = out_Pro_B.detach()\n",
    "Pro_B_pred = pred.cpu().numpy()\n",
    "for i in range(len(Pro_B_pred)):\n",
    "    Pro_B_pred[i] = -Pro_B_pred[i].max()\n",
    "Pro_B_pred = Pro_B_pred[:, 0]\n",
    "g = sns.displot(Pro_B_pred)\n",
    "g.fig.set_size_inches(5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if type(embedding_Pro_B) != np.ndarray:\n",
    "    embedding_Pro_B = embedding_Pro_B.cpu().numpy()\n",
    "    test_pred_labels = encoder_celltype.inverse_transform(test_pred_labels)\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "for color in np.unique(np.array(test_pred_labels)):\n",
    "    i = np.where(test_pred_labels == color)\n",
    "    ax.scatter(np.array(test_embeddings)[i,0], np.array(test_embeddings)[i,1], np.array(test_embeddings)[i,2], label=color)\n",
    "    \n",
    "ax.scatter(np.array(embedding_Pro_B)[:,0], np.array(embedding_Pro_B)[:,1], np.array(embedding_Pro_B)[:,2], label='Pro-B')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2348cd6694856b0cd2d6b81b695ed315114d0b15be6e262935473537dcdc1e9f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
